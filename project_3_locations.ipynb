{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Three:  Data Mining for Personal Information from Anonymous Users\n",
    "\n",
    "This is the secondary Jupyter notebook.  Using the best model from the first notebook's gridsearches, the remainder of the process (trying to determine a user's location based on their word choices) was conducted.\n",
    "\n",
    "---\n",
    "\n",
    "### Imports, function definitions, and data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#set up and processing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#estimators and transformers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pd(subreddit):\n",
    "    #This is a modified function from the previous notebook to get additional scrapes.  In this function, only comments are used, and it pulls deeper than the others.\n",
    "    url = f\"https://api.pushshift.io/reddit/search/comment\"\n",
    "    \n",
    "    params = {\n",
    "        \"subreddit\":subreddit,\n",
    "        \"size\":\"500\",\n",
    "        \"fields\":[\"author\", \"body\",\"subreddit\"]\n",
    "            }\n",
    "\n",
    "    a = 0\n",
    "    b = 1\n",
    "    posts = []\n",
    "\n",
    "    while b < 100:\n",
    "        params[\"after\"] = f\"{b}d\"\n",
    "        params[\"before\"] = f\"{a}d\"\n",
    "        res = requests.get(url, params)\n",
    "        data = res.json()\n",
    "        posts+= data[\"data\"]\n",
    "        a+=1\n",
    "        b+=1\n",
    "        time.sleep(1)\n",
    "    return pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial data pulls and assessment of results.\n",
    "nyc = get_pd(\"nyc\")\n",
    "bos = get_pd(\"boston\")\n",
    "chi = get_pd(\"chicago\")\n",
    "\n",
    "print(nyc.shape, bos.shape, chi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Like in the previous notebook, there were issues with using the .replace method with pointers (in functions or for loops) so each dataframe was manually cleaned and saved to a csv.\n",
    "### NYC cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc = nyc.replace(\"\", np.nan)\n",
    "nyc = nyc.replace(\"[removed]\",np.nan)\n",
    "nyc = nyc.replace(\"[deleted]\",np.nan)\n",
    "nyc.dropna(inplace=True)\n",
    "\n",
    "nyc.to_csv(\"./data_loc/nyc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = bos.replace(\"\", np.nan)\n",
    "bos = bos.replace(\"[removed]\",np.nan)\n",
    "bos = bos.replace(\"[deleted]\",np.nan)\n",
    "bos.dropna(inplace=True)\n",
    "\n",
    "bos.to_csv(\"./data_loc/bos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chicago cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi = chi.replace(\"\", np.nan)\n",
    "chi = chi.replace(\"[removed]\",np.nan)\n",
    "chi = chi.replace(\"[deleted]\",np.nan)\n",
    "chi.dropna(inplace=True)\n",
    "\n",
    "chi.to_csv(\"./data_loc/chi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4569, 3) (4594, 3) (4570, 3)\n"
     ]
    }
   ],
   "source": [
    "#Each of the three csvs are now used for drawing the dataframes from, rather than from the initial scrape. This prevents needing to scrape again if something goes awry.\n",
    "nyc = pd.read_csv(\"./data_loc/nyc.csv\")\n",
    "bos = pd.read_csv(\"./data_loc/bos.csv\")\n",
    "chi = pd.read_csv(\"./data_loc/chi.csv\")\n",
    "\n",
    "print(nyc.shape, bos.shape, chi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating the Model\n",
    "\n",
    "This uses all of the default KNeighbors and Count Vectorizer parameters, which in the gridsearches on the core notebook produced the highest predictive ability.  However, this is now a classification issue between three subreddits, so I retested it.  The model was still yielding around 17% above the baseline.  For targeted madss advertising, an especially high degree of accuracy is not always necessary (although it is always desirable) so I decided to continue with this model despite the relatively low degree of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([nyc, bos, chi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"body\"]\n",
    "y = df[\"subreddit\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:\t\t0.7941926087748042\n",
      "Test score:\t\t0.5154714233709501\n"
     ]
    }
   ],
   "source": [
    "pipe_nb = Pipeline([\n",
    "    (\"cvec\", CountVectorizer(max_df=0.9, max_features=20_000, min_df=1, stop_words=\"english\")),\n",
    "    (\"nb\", MultinomialNB())\n",
    "     ])\n",
    "\n",
    "pipe_nb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training score:\\t\\t{pipe_nb.score(X_train, y_train)}\")\n",
    "print(f\"Test score:\\t\\t{pipe_nb.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating the corpus of known-location users\n",
    "\n",
    "In this section, we will\n",
    "1.  Get a set of users for each city;\n",
    "2.  Pull in a corpus of documents from other miscellaneous subreddits; and \n",
    "3.  See what users match between the new corpus.  \n",
    "\n",
    "This means we will have a huge dataset of posts from multiple subreddits, among which SOME but not all of the authors have been identified by location.  We will then run our model on this new corpus and see if we can predict the location of these unknown users, using the matching author IDs from the city subreddits to check our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_chi = set(chi[\"author\"].tolist())\n",
    "auth_nyc = set(nyc[\"author\"].tolist())\n",
    "auth_bos = set(bos[\"author\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2144 2210 2210\n"
     ]
    }
   ],
   "source": [
    "print(len(auth_chi), len(auth_bos), len(auth_bos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = get_pd(\"funny\")\n",
    "gmg = get_pd(\"gaming\")\n",
    "sci = get_pd(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9800, 3) (9800, 3) (9800, 3)\n"
     ]
    }
   ],
   "source": [
    "fun = ask\n",
    "print(fun.shape, gmg.shape, sci.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean-up & concatenation\n",
    "\n",
    "Here, the three data frames are cleaned and then compiled into a single dataframe.  The subreddit column is unnecessary & is therefore dropped from the dataframe.  After all, we only need the authors and the words they use - their comments - from this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheDerpiestCat</td>\n",
       "      <td>u/repostsleuthbot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kmsae</td>\n",
       "      <td>Is this the episode of Curb where J.B. Smoove ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michael_Silveous</td>\n",
       "      <td>What?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uptokesforall</td>\n",
       "      <td>What was your answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>texxmix</td>\n",
       "      <td>I’m a grow man that’s always thought I was “be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9792</th>\n",
       "      <td>KodakKid3</td>\n",
       "      <td>Haha I totally get that, personally I’m a coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>pdgenoa</td>\n",
       "      <td>I appreciate that link, thanks. \\n\\nAnd I've h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9796</th>\n",
       "      <td>shroudoftheimmortal</td>\n",
       "      <td>Let me know when you find a habitable planet f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>Litis3</td>\n",
       "      <td>I don't have the section before me but it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>Vektor0</td>\n",
       "      <td>&amp;gt; desegregating means less whites and more ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26137 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                               body\n",
       "0          TheDerpiestCat                                  u/repostsleuthbot\n",
       "1                   kmsae  Is this the episode of Curb where J.B. Smoove ...\n",
       "2        Michael_Silveous                                              What?\n",
       "3           uptokesforall                               What was your answer\n",
       "4                 texxmix  I’m a grow man that’s always thought I was “be...\n",
       "...                   ...                                                ...\n",
       "9792            KodakKid3  Haha I totally get that, personally I’m a coll...\n",
       "9795              pdgenoa  I appreciate that link, thanks. \\n\\nAnd I've h...\n",
       "9796  shroudoftheimmortal  Let me know when you find a habitable planet f...\n",
       "9797               Litis3  I don't have the section before me but it was ...\n",
       "9798              Vektor0  &gt; desegregating means less whites and more ...\n",
       "\n",
       "[26137 rows x 2 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun = fun.replace(\"[removed]\",np.nan)\n",
    "fun = fun.replace(\"[deleted]\",np.nan)\n",
    "fun.dropna(inplace=True)\n",
    "\n",
    "gmg = gmg.replace(\"[removed]\",np.nan)\n",
    "gmg = gmg.replace(\"[deleted]\",np.nan)\n",
    "gmg.dropna(inplace=True)\n",
    "\n",
    "sci = sci.replace(\"[removed]\",np.nan)\n",
    "sci = sci.replace(\"[deleted]\",np.nan)\n",
    "sci.dropna(inplace=True)\n",
    "\n",
    "corpus = pd.concat([fun, gmg, sci])\n",
    "corpus.drop(columns=\"subreddit\", inplace=True)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting user cross-sections\n",
    "\n",
    "Here, we get the set intersections of the users from the new dataframe \"corpus\" and each of the three locational dataframes.  Despite the huge userbase in each category, there were surprisingly few overlaps in userbases (around 35 unique users from each city were posting in the other subreddits).  This led to some worries about model prediction quality, which I discuss further in the README and address the best I could with the model-running below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "34\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "auth_corpus = set(corpus[\"author\"].tolist())\n",
    "\n",
    "print(len(auth_corpus.intersection(auth_nyc)))\n",
    "print(len(auth_corpus.intersection(auth_chi)))\n",
    "print(len(auth_corpus.intersection(auth_bos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a dataframe of known-location users\n",
    "\n",
    "From \"corpus\", the new dataframe, we have ~100 users who *also* post in locational subreddits (specifically, r/boston, r/chicago, or r/nyc).  Therefore in order to test whether the model is *able to distinguish a user's location based on the content & words of their posts,* we compile a dataframe of these intersection users.  Then we can run the model we already constructed on this smaller dataframe and test to see whether or not it's actually working, because we already know what the outcome *should* be with these users.\n",
    "\n",
    "I am aware that there is almost certainly a better way to have gotten such a dataframe from \"corpus\" with Python, but at this point, time constraints led me to settle on the first solution that worked rather than the most elegant solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[\"loc\"] = corpus[\"author\"]\n",
    "corpus[\"loc\"] = corpus[\"loc\"].map(lambda x: \"nyc\" if x in auth_nyc else np.nan)\n",
    "nyc_corp = corpus.dropna()\n",
    "\n",
    "corpus[\"loc\"] = corpus[\"author\"]\n",
    "corpus[\"loc\"] = corpus[\"loc\"].map(lambda x: \"boston\" if x in auth_bos else np.nan)\n",
    "bos_corp = corpus.dropna()\n",
    "\n",
    "corpus[\"loc\"] = corpus[\"author\"]\n",
    "corpus[\"loc\"] = corpus[\"loc\"].map(lambda x: \"chicago\" if x in auth_chi else np.nan)\n",
    "chi_corp = corpus.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "known = pd.concat([nyc_corp, bos_corp, chi_corp])\n",
    "known = known.replace(\"AutoModerator\", np.nan)\n",
    "known.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the known data\n",
    "\n",
    "I ran the test initially once and got a score of around ~32%.  However, the thought occured to me that it was possible that the model just got unlucky with this run, so I tried running the model an arbitrarily large number of time with replacement, and got the mean score.  The big assumption here is that the posts I pulled from \"corpus\" are representative of the data I wanted to pick, which is unfortunately questionable given the relatively small amount of intersection users I had to work with.  There are a number of limits & concerns with the final outcome wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score:\t\t0.34004316546762586\n"
     ]
    }
   ],
   "source": [
    "meanlst = []\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    X = known[\"body\"].sample(len(known[\"body\"]), replace=True)\n",
    "    y = known[\"loc\"].sample(len(known[\"loc\"]), replace=True)\n",
    "    meanlst.append(pipe_nb.score(X, y))\n",
    "    \n",
    "print(f\"Final score:\\t\\t{np.mean(meanlst)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
